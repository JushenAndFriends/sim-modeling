
<!-- keywords:聚类;机器学习;讲义;Matlab;k-means; -->
<!-- description:这里讲了最基本的K-means聚类的方法和基本原理以及matlab中的用法。 -->

#  数据科学入门2.4：基本的聚类

![](2020-03-05-10-58-16.png)

## 啥是聚类

聚类是一种非监督学习，他和分类不一样，分类的训练数据中每个样本是有标签标注了他是属于哪一类的，我们明确的是知道有哪几类。而聚类不一样，我们只有一堆数据，并不知道这些训练数据分别属于哪一类别，甚至完全被不知道有哪些类别（有时候可以知道）。聚类就是根据这些样本的特征把比较像的归为一类。比如你上网易云音乐或者什么别的软件的时候，他的推荐算法会对你进行聚类，把你和某些人归为一类从而给你推荐这些人喜欢的歌曲。

聚类的方法有很多种，有一种是基于距离第，也就是我们今天要讲的k-means这种。还记得我们以前讲的一个样本的特征就是向量，这些向量可以构成一个特征空间。基于距离的聚类就是把这些空间中距离比较近的点归为一类。

![](2020-03-05-11-03-39.png)

除了基于距离的还有基于hierarchy的。就是他找这些样本的上一级，如果他们的上一级或者上几级是同一个就是一类的。比如你们两个的曾祖父是同一个人，那你们就被归为一家人。

我们今天只讲简单的K-means，其他的大家可以下去自学。

## k-means clustering的基本原理

之前讲过了k-means是基于距离的，Means就是平均，就是几个点距离的平均的意思，也就是这些点的中心（centroid），K是分为几类。我们们用1为数据举个例子：

![](2020-03-05-11-04-15.png)

1. 确定分为几类，k=？我们这里让k=3
2. 随便选择3个样本点作为3个类型的重心

![](2020-03-05-11-07-24.png)

3. 计算每个点到这些重心的距离，离哪个近就属于哪一类

![](2020-03-05-11-10-56.png)

4. 根据这个聚类的结果，重新计算每一个类型的重心，可以看到目前聚类结果不咋样

![](2020-03-05-11-13-24.png)

5. 用这个新的重心重复3-4步，直到重心不再变化位置

![](wkmeans_1.gif)

6. 评价这个聚类的得分：
$$
   se=\sum_{k=1}^{Clusters}\sum_{i=1}^{n}D_{ki}
$$

这个就是总的误差，就是每个点到他自身cluster重心的距离求和，再k一定的情况下，这个值越小越好。k-means即使取同一个k，由于初始的重心是随机的，每次算一遍结果也可能会不同。但要注意这个k只能在se只能在同一个k的情况下比较，显然随着k的增大，se会减小的，假如k=n，就是每个点自身就是一类，那中心就是本身，se=0。

### K如何取值

这个有很多种办法，最常见的就是根据我们的目标。比如把学生分为听话的和不听话的，那么K就是2，把明星分为男、女、不男不女，那么k=3。

有些事后我们啥也不知道，怎么办？那就用elbow method。这是怎么玩呢？我们就让k=2开始增加k的值，不断地做clustering，计算se，并且把他画出来。当se下降的最大到se下降平缓的时候那个拐点，就是我们要的k值。因为当增加k的时候误差减小的小，说明我们在分类并没有解释更多误差，增加k分类的意义不大。

![](2020-03-05-12-49-00.png)

这里可以看到，我们想要的k值就是3。

ok，在matlab里面k-measn很简单，大家可以把我们上次做的iris分类的例子去掉label，也就是species这个列，试试。

https://www.mathworks.com/help/stats/kmeans.html

这里是这个函数的介绍，我在罗嗦一点：

```
[idx,C,sumd,D] = kmeans(X,k)
```

1. 假设X是n*m矩阵，n行样本，m个特征
2. idx就是聚类结果，n*1，每一个代表每个样本的类型
3. C是重心，k个cluster的重心，k*m大小
4. sumd，是每个点到其所在cluster重心的距离求和。k*1个
5. D，是每个点到其所在cluster重心的距离， k*n个

要求完成聚类和elbow method，画图，好弱智把，可加入pca啥的。

![](2020-03-05-12-58-10.png)

![](2020-03-05-12-58-47.png)